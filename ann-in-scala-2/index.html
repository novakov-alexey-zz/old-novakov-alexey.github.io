<!DOCTYPE html>
<html lang="en">
    <head>
      
        <meta http-equiv="X-UA-Compatible" content="IE=edge" >
        <meta http-equiv="content-type" content="text/html; charset=utf-8">   
        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@alexey_novakov">
        <meta name="twitter:creator" content="@alexey_novakov">
        <meta name="twitter:url" content="https://novakov-alexey.github.io">
        <meta name="twitter:title" content="SE Notes - Alexey Novakov">
        <meta name="twitter:description" content="Software Engineering notes on Scala, JVM, Rust, Cloud and other goodies">
        <meta name="twitter:image" content="https://novakov-alexey.github.io/img/alexey_white2.jpg">
        <meta name="twitter:image:alt" content="Alexey Novakov photo for personal web-site">

        <!-- SEO -->
        
        <title> Artificial Neural Network in Scala - part 2 | Software Engineering Notes </title>
        

        <!-- Enable responsiveness on mobile devices-->
        <meta name="viewport"  content="width=device-width, initial-scale=1.0, maximum-scale=1" >

        <!--  css -->
        <link rel="stylesheet"  href="https:&#x2F;&#x2F;novakov-alexey.github.io/ergo.css" >

        <!-- fonts -->
        <!-- preload  -->
        <link rel="preload" href="https://fonts.googleapis.com/css?family=Montserrat:200,300,300i,400,500,500i,600,700,800,900|Raleway" as="style">
        <link rel="preload"  href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous" as="style" >
        <!-- load -->
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:200,300,300i,400,500,500i,600,700,800,900|Raleway">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"  integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
      
    </head>

    <body>
      
        
<input type="checkbox" id="openSidebarMenu" class="openSidebarMenu">
<label class="menu cross menu--1" for="openSidebarMenu">
    <svg viewBox="0 0 75 75" xmlns="https://www.w3.org/2000/svg">
        <circle cx="50" cy="50" r="30" />
        <path class="line--1" d="M0 40h62c13 0 6 28-4 18L35 35" />
        <path class="line--2" d="M0 50h70" />
        <path class="line--3" d="M0 60h62c13 0 6-28-4-18L35 65" />
    </svg>
</label>

<div id="sidebarMenu">
    <div class="menu_wrapper">
        <ul class="sidebarMenuInner" >     
            <li>
                <a href="https:&#x2F;&#x2F;novakov-alexey.github.io" >Software Engineering Notes</a>
                <div class="menu_div" ></div>
            </li>                   
            <li><a href="https:&#x2F;&#x2F;novakov-alexey.github.io/cv/">CV</a></li>
            <li><a href="https:&#x2F;&#x2F;novakov-alexey.github.io/presentations">Presentations</a></li>
            <li><a href="https:&#x2F;&#x2F;novakov-alexey.github.io/categories">Categories</a></li>
            <li><a href="https:&#x2F;&#x2F;novakov-alexey.github.io/tags">Tags</a></li>
        </ul>
    </div>
</div>

      

      
<nav id="overlord" class="overlord" >
  
<figure class="mini_logo ">
    <a href="https:&#x2F;&#x2F;novakov-alexey.github.io" style="background-image: url(https:&#x2F;&#x2F;novakov-alexey.github.io/img/alexey_white2.jpg)"></a>
</figure>
<h5>
    <a href="https:&#x2F;&#x2F;novakov-alexey.github.io">Software Engineering Notes</a>
</h5>

</nav>

<section class="post_container">
  <article>
    
      <h1 class="article_title"><a href="https:&#x2F;&#x2F;novakov-alexey.github.io&#x2F;ann-in-scala-2&#x2F;" id="article_link">Artificial Neural Network in Scala - part 2</a></h1>
      
<ul class="frontmatter frontmatter_page" id="frontmatter">
    <li>
        <time class="article_time"  datetime="2021-02-05">February 05, 2021</time>
    </li>
    <span class="dotDivider"></span>
    <li> 3864 words </li>
    <span class="dotDivider" ></span>
    <li> 20 min </li>
</ul>

      
      <div class="post-toc" id="post-toc">
          <h3 class="post-toc-title">Contents</h3>
          <div class="post-toc-content always-active">
              <nav id="TableOfContents">
                  <ul>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#tensor-library" class="toc-link">Tensor Library</a>
                          
                          <ul>
                              
                              <li>
                                  <a href="https://novakov-alexey.github.io/ann-in-scala-2/#matmul" class="toc-link">Matmul</a>
                              </li>
                              
                          </ul>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#neural-network-dsl" class="toc-link">Neural Network DSL</a>
                          
                          <ul>
                              
                              <li>
                                  <a href="https://novakov-alexey.github.io/ann-in-scala-2/#model-initialisation" class="toc-link">Model initialisation</a>
                              </li>
                              
                              <li>
                                  <a href="https://novakov-alexey.github.io/ann-in-scala-2/#training-loop" class="toc-link">Training loop</a>
                              </li>
                              
                          </ul>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#data-preparation" class="toc-link">Data Preparation</a>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#training-run" class="toc-link">Training Run</a>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#testing" class="toc-link">Testing</a>
                          
                          <ul>
                              
                              <li>
                                  <a href="https://novakov-alexey.github.io/ann-in-scala-2/#single-test" class="toc-link">Single Test</a>
                              </li>
                              
                              <li>
                                  <a href="https://novakov-alexey.github.io/ann-in-scala-2/#dataset-test" class="toc-link">Dataset Test</a>
                              </li>
                              
                          </ul>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#python-implementation" class="toc-link">Python Implementation</a>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#summary" class="toc-link">Summary</a>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#source-code" class="toc-link">Source Code</a>
                          
                      </li>
                      
                      <li>
                          <a href="https://novakov-alexey.github.io/ann-in-scala-2/#reference-links" class="toc-link">Reference Links</a>
                          
                      </li>
                      
                  </ul>
              </nav>
          </div>
      </div>
            
      <p>In this article we are going to implement ANN from scratch in Scala. It is continuation of <a href="../ann-in-scala-1">the first article</a>, which describes 
a theory of ANN.</p>
<p>This implementation will consist of:</p>
<ol>
<li>Mini-library for sub-set of Tensor calculus</li>
<li>Mini-library for data preparation</li>
<li>A DSL for Neural Network creation, including layers</li>
<li>Pluggable weights optimizer</li>
<li>Pluggable implementation of activation and loss functions</li>
<li>Pluggable training metric calculation</li>
</ol>
<p>Everything will be implemented in pure Scala without using any third-party code. 
By pluggable I mean extendable, i.e. a user can provide own implementation by implementing Scala trait.</p>
<p>Neural network and data preprocessing APIs are inspired by <a href="https://keras.io/">Keras</a> and <a href="https://scikit-learn.org/stable/">scikit-learn</a> libraries.</p>
<h1 id="tensor-library">Tensor Library</h1>
<p>Before starting our journey into the world of linear algebra we need good support for Tensor calculus such as
multiplication, addition, subtraction, transponding operations. Without these operations, we will clutter the
main algorithm so that another person, who will be reading our code, may lost. It is very easy to be blown
away by pile of code which is trying to mimic math. Scala is perfect language to implement math expression as
it supports custom operands by using symbols as definitions (variables, methods, etc.), i.e. we can implement &quot;*&quot; or any other math operations as part of our custom type <code>Tensor</code>.</p>
<p>Below we define <code>Tensor</code> trait for a generic type <code>T</code>. Later, we will set boundaries for T. It must have <code>given</code> 
instances of ClassTag and Numeric types for array creation and general numerical computations.</p>
<pre style="background-color:#282a36;">
<code><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="color:#ff79c6;">type </span><span style="color:#6be5fd;">A
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">data</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">A
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">length</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int
  def </span><span style="color:#50fa7b;">sizes</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#8be9fd;">Int</span><span style="color:#f8f8f2;">]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">cols</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int</span><span style="color:#f8f8f2;">
  
extension [</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](t: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">])
    </span><span style="color:#6272a4;">// dot product    
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">*</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">that</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">mul(t, that)
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">map</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">f</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T </span><span style="color:#ff79c6;">=&gt; </span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">map[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">](t, f)
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">-</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">that</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">subtract(t, </span><span style="color:#6be5fd;">Tensor0D</span><span style="color:#f8f8f2;">(that))
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">-</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">that</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">subtract(t, that)
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">+</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">that</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">plus(t, that)    
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">sum</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">sum(t)        
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">split</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">fraction</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">): (</span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]) </span><span style="color:#ff79c6;">= 
        </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">split(fraction, t)
    </span><span style="color:#6272a4;">// Hadamard product
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">multiply</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">that</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">multiply(t, that)
    </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">batches</span><span style="color:#f8f8f2;">(
        </span><span style="font-style:italic;color:#ffb86c;">batchSize</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int
    </span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">Iterator</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Tensor</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">batches(t, batchSize)    
</span></code></pre>
<p>In extension section we add lots of operations that our generic Tensor is going to support. Some of them are symbolic like
<code>*</code> and <code>-</code>. Other operations are more traditional methods such as <code>map</code> or <code>sum</code>. 
Note that <code>*</code> and <code>multiply</code> are two different operations. From math perspective, the first one is a <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>
another one is a <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a>. Most of the time, we will use &quot;dot product&quot; operation, 
however in one place Hadamard product is going to be used (back-propagation part).</p>
<p>All extension methods are delegating the operations to plain Scala functions in the Tensor singleton object.</p>
<p>Before checking some of the implementations for Tensor operations, let's look on 3 cases of Tensor itself.</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Tensor0D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">extends </span><span style="text-decoration:underline;font-style:italic;color:#8be9fd;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="color:#ff79c6;">type </span><span style="color:#6be5fd;">A </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">T
  </span><span style="color:#ff79c6;">....

</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Tensor1D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]) </span><span style="color:#ff79c6;">extends </span><span style="text-decoration:underline;font-style:italic;color:#8be9fd;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="color:#ff79c6;">type </span><span style="color:#6be5fd;">A </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
  </span><span style="color:#ff79c6;">....

</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]) </span><span style="color:#ff79c6;">extends </span><span style="text-decoration:underline;font-style:italic;color:#8be9fd;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="color:#ff79c6;">type </span><span style="color:#6be5fd;">A </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]  
  </span><span style="color:#ff79c6;">....
</span></code></pre>
<p>Look at how <code>A</code> type is set based on the Tensor dimension.</p>
<p>From math perspective, first instance is a scalar number, second is a vector and third is a matrix. Of course, we could implement
tensors in more generic way and invent some N-dimensional array that would support 3, 4 and any number of dimensions,
but I think from our personal learning perspective, making more concrete hard-coded classes would be easier to understand the whole ANN 
implementation.</p>
<h2 id="matmul">Matmul</h2>
<p>Let's look only at one important operation from Tensor API which is dot product.</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">mul</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">a</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">b</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=
  </span><span style="color:#f8f8f2;">(a, b) </span><span style="color:#ff79c6;">match
    case </span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Tensor0D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">), </span><span style="font-style:italic;color:#ffb86c;">t</span><span style="color:#f8f8f2;">) </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;">
      scalarMul(t, data)
    </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">t</span><span style="color:#f8f8f2;">, </span><span style="color:#6be5fd;">Tensor0D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;">
      scalarMul(t, data)
    </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Tensor1D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">), </span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data2</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
      </span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(matMul(asColumn(data), data2))
    </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">), </span><span style="color:#6be5fd;">Tensor1D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data2</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
      </span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(matMul(data, asColumn(data2)))
    </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Tensor1D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">), </span><span style="color:#6be5fd;">Tensor1D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data2</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
      </span><span style="color:#6be5fd;">Tensor1D</span><span style="color:#f8f8f2;">(matMul(asColumn(data), </span><span style="color:#6be5fd;">Array</span><span style="color:#f8f8f2;">(data2))</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">head)
    </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">), </span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">data2</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
      </span><span style="color:#6be5fd;">Tensor2D</span><span style="color:#f8f8f2;">(matMul(data, data2))

  </span><span style="color:#ff79c6;">private </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">matMul</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#f8f8f2;">](
      </span><span style="font-style:italic;color:#ffb86c;">a</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
      </span><span style="font-style:italic;color:#ffb86c;">b</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]
  ): </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]] </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;">
    assert(
      a</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">head</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length </span><span style="color:#ff79c6;">==</span><span style="color:#f8f8f2;"> b</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length,
      </span><span style="color:#f1fa8c;">&quot;The number of columns in the first matrix should be equal &quot;</span><span style="color:#f8f8f2;"> + 
      </span><span style="color:#8be9fd;">s</span><span style="color:#f1fa8c;">&quot;to the number of rows in the second, ${</span><span style="color:#f8f8f2;">a</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">head</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length</span><span style="color:#f1fa8c;">} != ${</span><span style="color:#f8f8f2;">b</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length</span><span style="color:#f1fa8c;">}&quot;
    </span><span style="color:#f8f8f2;">)
    </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">rows </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> a</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length
    </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">cols </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> colsCount(b)
    </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">res </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Array</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">ofDim[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">](rows, cols)

    </span><span style="color:#ff79c6;">for</span><span style="color:#f8f8f2;"> i &lt;- (</span><span style="color:#bd93f9;">0</span><span style="color:#f8f8f2;"> until rows)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">indices </span><span style="color:#ff79c6;">do
      for</span><span style="color:#f8f8f2;"> j &lt;- (</span><span style="color:#bd93f9;">0</span><span style="color:#f8f8f2;"> until cols)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">indices </span><span style="color:#ff79c6;">do
        </span><span style="font-style:italic;color:#8be9fd;">var </span><span style="color:#ffffff;">sum </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> summon[</span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">zero
        </span><span style="color:#ff79c6;">for</span><span style="color:#f8f8f2;"> k &lt;- b</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">indices </span><span style="color:#ff79c6;">do</span><span style="color:#f8f8f2;">
          sum </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> sum + (a(i)(k) * b(k)(j))
        res(i)(j) </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> sum
    res    
</span></code></pre>
<ol>
<li>First we select specific type of multiplication based on the tensor dimension. </li>
<li>If tensor is not scalar, then we try to use matrix multiplication. Here, if some of the operands is vector we make that vector as matrix
with one column according to math convention. </li>
<li>Later we check operands dimensions, as they must obey rules of
matrix multiplication. If rules are not met we throw an error. No Scala <code>Either</code> type or other error modelling is used to not clutter the code. Our goal is to stay as close as possible to math and keep balance between using types and readability.</li>
</ol>
<p>See <a href="https://github.com/novakov-alexey/deep-learning-scala/blob/master/src/main/scala/tensor.scala">source code on GitHub</a> for full implementation of tensor library.</p>
<h1 id="neural-network-dsl">Neural Network DSL</h1>
<p>Let's define a trait for abstract model that can learn:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> Model</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">train</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">y</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">epochs</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int</span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">Model</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">predict</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">reset</span><span style="color:#f8f8f2;">(): </span><span style="font-style:italic;color:#66d9ef;">Model</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">currentWeights</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">losses</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
</span></code></pre>
<p>The first two methods are the main ones. </p>
<ol>
<li>We can use <code>train</code> to provide input features as <code>x</code> and target values as <code>y</code>, specify
number of training cycles as <code>epochs</code> to learn the right parameters for future predictions.</li>
<li><code>predict</code> allows us to infer target value by giving only <code>x</code> data</li>
<li><code>reset</code> cleans model weights, so that it initialises them again upon next training</li>
<li><code>currentWeights</code> and <code>losses</code> are returning weights and losses of the last training cycle.</li>
</ol>
<p>Machine learning model is a stateful thing. It keeps list of parameters called weights and biases of type <code>List[Weight[T]]</code>.
These parameters are the heart of the model. They are mutating on every training epoch and data batch.</p>
<h2 id="model-initialisation">Model initialisation</h2>
<p>Before designing neural network training API, let's look at entities we need:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> ActivationFunc</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">extends </span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=&gt; </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]):
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">apply</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">derivative</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]

</span><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> Loss</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">apply</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">actual</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">predicted</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">T

</span><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> Layer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">units</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int
  def </span><span style="color:#50fa7b;">f</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">ActivationFunc</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]

</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Dense</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">f</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">ActivationFunc</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">units</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">extends </span><span style="text-decoration:underline;font-style:italic;color:#8be9fd;">Layer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]

</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">](
  </span><span style="font-style:italic;color:#ffb86c;">w</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">b</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], 
  </span><span style="font-style:italic;color:#ffb86c;">f</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">ActivationFunc</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">units</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int
</span><span style="color:#f8f8f2;">)

</span><span style="color:#6272a4;">/*
 * z - before activation = w * x
 * a - activation value
 */
</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Activation</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">](</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">z</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">a</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">])
</span></code></pre>
<p>We have modelled network parameters as traits with implementations as case classes. Later we use 
them to create an instance of the model.</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> RandomGen</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">gen</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T

</span><span style="font-style:italic;color:#ff79c6;">case class</span><span style="text-decoration:underline;color:#8be9fd;"> Sequential</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">RandomGen</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Optimizer</span><span style="color:#f8f8f2;">](
    </span><span style="font-style:italic;color:#ffb86c;">lossFunc</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Loss</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">],
    </span><span style="font-style:italic;color:#ffb86c;">learningRate</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">,
    </span><span style="font-style:italic;color:#ffb86c;">metric</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Metric</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">],
    </span><span style="font-style:italic;color:#ffb86c;">batchSize</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">16</span><span style="color:#f8f8f2;">,
    </span><span style="font-style:italic;color:#ffb86c;">weightStack</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int </span><span style="color:#ff79c6;">=&gt; </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]] </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(</span><span style="color:#bd93f9;">_</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int</span><span style="color:#f8f8f2;">) </span><span style="font-style:italic;color:#8be9fd;">=&gt; </span><span style="color:#6be5fd;">List</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">empty[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],    
    </span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Nil</span><span style="color:#f8f8f2;">,
    </span><span style="font-style:italic;color:#ffb86c;">losses</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Nil
</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">extends </span><span style="text-decoration:underline;font-style:italic;color:#8be9fd;">Model</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]:

  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">add</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">layer</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Layer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]): </span><span style="font-style:italic;color:#66d9ef;">Sequential</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;">
    copy(weightStack </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">inputs</span><span style="color:#f8f8f2;">) </span><span style="font-style:italic;color:#8be9fd;">=&gt; </span><span style="color:#ffffff;">{
      </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">currentWeights </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> weightStack(inputs)
      </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">prevInput </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;">
        currentWeights</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">reverse</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">headOption</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">map(</span><span style="color:#bd93f9;">_</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">units)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">getOrElse(inputs)
      </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">w </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> random2D(prevInput, layer</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">units)
      </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">b </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> zeros(layer</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">units)
      (currentWeights :+ </span><span style="color:#6be5fd;">Weight</span><span style="color:#f8f8f2;">(w, b, layer</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">f, layer</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">units))
    </span><span style="color:#ffffff;">}</span><span style="color:#f8f8f2;">)
</span></code></pre>
<p>There are bunch of parameters that we need in simple sequential model with fully connected layers:</p>
<ol>
<li>Generic type <code>T</code> is numeric type of the data which can be <code>Float</code>, <code>Double</code>, <code>Int</code>, etc. Most of the time you want numbers with floating point in machine learning.</li>
<li>Random generator can be provided as contextual abstraction (given instance). It is used to initialise
weights and biases for every layer.</li>
<li>Generic <code>U</code> is a type of optimisation algorithm that we use in back-propagation part of the training cycle. Also given as type class instance.</li>
<li><code>learningRate</code> and <code>batchSize</code> are hyper-parameters to be tuned externally.</li>
<li><code>weightStack</code> is a function that construct list of initial layers based on the provided earlier Layer configuration via
method <code>add</code>.  It is not supposed to be called manually. The <code>weightStack</code> function is called by <code>train</code> 
method internally to create initial list of weights, if weights are empty. If they 
are not empty, they are reused.</li>
</ol>
<p>This is how a user is supposed to use such API:</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">accuracy </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> accuracyMetric[</span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">]
  
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">ann </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Sequential</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">SimpleGD</span><span style="color:#f8f8f2;">](
  binaryCrossEntropy,
  learningRate </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">0.05</span><span style="font-style:italic;color:#8be9fd;">f</span><span style="color:#f8f8f2;">,
  metric </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> accuracy,
  batchSize </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">32
</span><span style="color:#f8f8f2;">)
  </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">add(</span><span style="color:#6be5fd;">Dense</span><span style="color:#f8f8f2;">(relu, </span><span style="color:#bd93f9;">6</span><span style="color:#f8f8f2;">))
  </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">add(</span><span style="color:#6be5fd;">Dense</span><span style="color:#f8f8f2;">(relu, </span><span style="color:#bd93f9;">6</span><span style="color:#f8f8f2;">))    
  </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">add(</span><span style="color:#6be5fd;">Dense</span><span style="color:#f8f8f2;">(sigmoid))
</span></code></pre>
<p>There is a type <code>SimpleGD</code> that picks up a required instance of <code>Optimizer</code> implementation. See details below.</p>
<h2 id="training-loop">Training loop</h2>
<p><code>train</code> method runs <code>trainEpoch</code> multiple times, which is equal to <code>epochs</code> parameter. 
Every training epoch returns new weights list, which is used again for the next epoch. This loop may run, for example, 100 times.
Also, we collect a list of average loss values and print a user metric value. We have set <code>accuracy</code> metric as per code earlier.</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">train</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">y</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="font-style:italic;color:#ffb86c;">epochs</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#8be9fd;">Int</span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">Model</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=
  lazy </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">inputs </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> x</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">cols
  </span><span style="color:#ff79c6;">lazy </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">actualBatches </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> y</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">batches(batchSize)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">toArray
  </span><span style="color:#ff79c6;">lazy </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">xBatches </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> x</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">batches(batchSize)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">zip(actualBatches)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">toArray
  </span><span style="color:#ff79c6;">lazy </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">w </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> getWeights(inputs)

  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#f8f8f2;">(</span><span style="color:#ffffff;">updatedWeights</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">epochLosses</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">=
    </span><span style="color:#f8f8f2;">(</span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;"> to epochs)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">foldLeft((w, </span><span style="color:#6be5fd;">List</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">empty[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">])) </span><span style="color:#ffffff;">{
      </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">((</span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">losses</span><span style="color:#f8f8f2;">), </span><span style="font-style:italic;color:#ffb86c;">epoch</span><span style="color:#f8f8f2;">) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
        val </span><span style="color:#f8f8f2;">(</span><span style="color:#ffffff;">w</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">avgLoss</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">metricValue</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> trainEpoch(xBatches, weights)
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">metricAvg </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> metric</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">average(x</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">length, metricValue)
        println(
          </span><span style="color:#8be9fd;">s</span><span style="color:#f1fa8c;">&quot;epoch: </span><span style="color:#ffffff;">$epoch</span><span style="color:#f1fa8c;">/</span><span style="color:#ffffff;">$epochs</span><span style="color:#f1fa8c;">, avg. loss: </span><span style="color:#ffffff;">$avgLoss</span><span style="color:#f1fa8c;">, </span><span style="background-color:#ff79c6;color:#f8f8f0;">
</span><span style="color:#f1fa8c;">          ${</span><span style="color:#f8f8f2;">metric</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">name</span><span style="color:#f1fa8c;">}: </span><span style="color:#ffffff;">$metricAvg</span><span style="color:#f1fa8c;">&quot;
        </span><span style="color:#f8f8f2;">)
        (w, losses :+ avgLoss)
    </span><span style="color:#ffffff;">}</span><span style="color:#f8f8f2;">
  copy(weights </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> updatedWeights, losses </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> epochLosses)
</span></code></pre>
<p><code>trainEpoch</code> is implementing forward- and back-propagation for every data sample batch:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#ff79c6;">private </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">trainEpoch</span><span style="color:#f8f8f2;">(
    </span><span style="font-style:italic;color:#ffb86c;">batches</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[(</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]], </span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Array</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]])],
    </span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]
) </span><span style="color:#ff79c6;">=
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#f8f8f2;">(</span><span style="color:#ffffff;">w</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">l</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">metricValue</span><span style="color:#f8f8f2;">) </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;">
    batches</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">foldLeft(weights, </span><span style="color:#6be5fd;">List</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">empty[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">], </span><span style="color:#bd93f9;">0</span><span style="color:#f8f8f2;">) </span><span style="color:#ffffff;">{
      </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">((</span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">batchLoss</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">metricAcc</span><span style="color:#f8f8f2;">), (</span><span style="font-style:italic;color:#ffb86c;">xBatch</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">yBatch</span><span style="color:#f8f8f2;">)) </span><span style="font-style:italic;color:#8be9fd;">=&gt;
        </span><span style="color:#6272a4;">// forward
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">activations </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> activate(xBatch</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">as2D, weights)
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">actual </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> yBatch</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">as2D          
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">predicted </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> activations</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">last</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">a          
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">error </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> predicted - actual          
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">loss </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> lossFunc(actual, predicted)

        </span><span style="color:#6272a4;">// backward
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">updated </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> summon[</span><span style="font-style:italic;color:#66d9ef;">Optimizer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">]]</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">updateWeights(
          weights,
          activations,
          error,
          learningRate
        )
        </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">metricValue </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> metric</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">calculate(actual, predicted)
        (updated, batchLoss :+ loss, metricAcc + metricValue)
    </span><span style="color:#ffffff;">}    
  </span><span style="color:#f8f8f2;">(w, getAvgLoss(l), metricValue)
</span></code></pre><h3 id="gradient-descent-optimizer">Gradient Descent Optimizer</h3>
<p>Now let's look at optimizer code. It implements standard gradient descent algorithm:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#ff79c6;">sealed </span><span style="font-style:italic;color:#ff79c6;">trait</span><span style="text-decoration:underline;color:#8be9fd;"> Optimizer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">U</span><span style="color:#f8f8f2;">]:
  </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">updateWeights</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](
    </span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
    </span><span style="font-style:italic;color:#ffb86c;">activations</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Activation</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
    </span><span style="font-style:italic;color:#ffb86c;">error</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">],
    </span><span style="font-style:italic;color:#ffb86c;">learningRate</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T
  </span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]

</span><span style="font-style:italic;color:#8be9fd;">type </span><span style="color:#f8f8f2;">SimpleGD
</span></code></pre>
<p>In order to update weights an optimizer needs:</p>
<ol>
<li>the list of weights to update</li>
<li>current activations for all layers</li>
<li>calculated error: yHat vs. y</li>
<li>learningRate parameter, which is static for the entire training cycle</li>
</ol>
<p><code>SimpleGD</code> type is a token to summon an optimizer instance. In future, we can extend optimizers with other algorithms.</p>
<p>Data batching is happening outside of the optimizer, in the <code>train</code> method.
We can select either full batch or mini-batch training by specifying a number of records in the batch. 
So that is why this optimizer is not specific type of gradient descent (stochastic, mini-batch, batch), but just works with 
whatever weights are given for updates.</p>
<p>Actual implementation of the gradient descent optimization:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#f8f8f2;">given </span><span style="color:#6be5fd;">Optimizer</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">SimpleGD</span><span style="color:#f8f8f2;">] </span><span style="background-color:#ff79c6;color:#f8f8f0;">with</span><span style="color:#f8f8f2;">
  </span><span style="color:#ff79c6;">override </span><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">updateWeights</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](
      </span><span style="font-style:italic;color:#ffb86c;">weights</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
      </span><span style="font-style:italic;color:#ffb86c;">activations</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Activation</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
      </span><span style="font-style:italic;color:#ffb86c;">error</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">],
      </span><span style="font-style:italic;color:#ffb86c;">learningRate</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">T
  </span><span style="color:#f8f8f2;">): </span><span style="font-style:italic;color:#66d9ef;">List</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]] </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;">      
    weights
      </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">zip(activations)
      </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">foldRight(
        </span><span style="color:#6be5fd;">List</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">empty[</span><span style="font-style:italic;color:#66d9ef;">Weight</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]],
        error,
        </span><span style="color:#6be5fd;">None</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Option</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">Tensor</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]]
      ) </span><span style="color:#ffffff;">{
        </span><span style="color:#ff79c6;">case </span><span style="color:#f8f8f2;">(
              (</span><span style="color:#6be5fd;">Weight</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">w</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">b</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">f</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">u</span><span style="color:#f8f8f2;">), </span><span style="color:#6be5fd;">Activation</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">z</span><span style="color:#f8f8f2;">, </span><span style="color:#bd93f9;">_</span><span style="color:#f8f8f2;">)),
              (</span><span style="font-style:italic;color:#ffb86c;">ws</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">prevDelta</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">prevWeight</span><span style="color:#f8f8f2;">)
            ) </span><span style="font-style:italic;color:#8be9fd;">=&gt;            
          val </span><span style="color:#ffffff;">delta </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(prevWeight </span><span style="color:#ff79c6;">match </span><span style="color:#ffffff;">{
            </span><span style="color:#ff79c6;">case </span><span style="color:#6be5fd;">Some</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">pw</span><span style="color:#f8f8f2;">) </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;"> prevDelta * pw</span><span style="color:#ff79c6;">.</span><span style="color:#6be5fd;">T
            </span><span style="color:#ff79c6;">case </span><span style="color:#6be5fd;">None     </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;"> prevDelta
          </span><span style="color:#ffffff;">}</span><span style="color:#f8f8f2;">) multiply f</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">derivative(z)

          </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">partialDerivative </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> x</span><span style="color:#ff79c6;">.</span><span style="color:#6be5fd;">T</span><span style="color:#f8f8f2;"> * delta
          </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">newWeight </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> w - (learningRate * partialDerivative)
          </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">newBias </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> b - (learningRate * delta</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">sum)
          </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">updated </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">Weight</span><span style="color:#f8f8f2;">(newWeight, newBias, f, u) +: ws
          (updated, delta, </span><span style="color:#6be5fd;">Some</span><span style="color:#f8f8f2;">(w))
      </span><span style="color:#ffffff;">}
      </span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">_1
</span></code></pre>
<p>The weights update starts from tail and moves to the head of the list, i.e. from the last layer to the first hidden layer.
<code>weights</code> and <code>activations</code> are equal in length, since the last one is produced via the weight list during the forward-propagation. </p>
<p>The complex part is calculating the <code>delta</code> that we use for partial derivative. </p>
<ol>
<li>Initial <code>delta</code> is equal to <code>error</code>. Next layer is calculating <code>delta</code> on its own, which is a dot product of previous layer <code>delta</code> and <code>weights</code>.</li>
<li>Last layer does not have previous weights.</li>
<li>Every <code>delta</code> is then multiplied by activation function derivative: <code>f.derivative(z)</code>.</li>
<li>The rest part is simpler and more or less linear. We calculate <code>partialDerivative</code> and update layer weights and biases.</li>
<li>We pass current layer weight and delta to the next layer. Usage of <code>foldRight</code> helps us easily to pass these parameters to the next layer.</li>
</ol>
<p>This folding loop returns updated list of weights, which is, of course, has equal length comapre to the original list.</p>
<h1 id="data-preparation">Data Preparation</h1>
<p>Before we start learning, we need to prepare initial data for the training. 
Unfortunately, data preparation requires us quite a lot of code to write. </p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">def </span><span style="color:#50fa7b;">createEncoders</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">Numeric</span><span style="color:#ff79c6;">: </span><span style="font-style:italic;color:#66d9ef;">ClassTag</span><span style="color:#f8f8f2;">](
    </span><span style="font-style:italic;color:#ffb86c;">data</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">]
  ): </span><span style="font-style:italic;color:#66d9ef;">Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=&gt; </span><span style="font-style:italic;color:#66d9ef;">Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">] </span><span style="color:#ff79c6;">=
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">encoder </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">LabelEncoder</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">]()</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">fit(data</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">col(</span><span style="color:#bd93f9;">2</span><span style="color:#f8f8f2;">))
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">hotEncoder </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">OneHotEncoder</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">]()</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">fit(data</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">col(</span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">))
  
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">label </span><span style="color:#ff79c6;">= </span><span style="font-style:italic;color:#ffb86c;">x </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;"> encoder</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">transform(x, </span><span style="color:#bd93f9;">2</span><span style="color:#f8f8f2;">)
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">hot </span><span style="color:#ff79c6;">= </span><span style="font-style:italic;color:#ffb86c;">x </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;"> hotEncoder</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">transform(x, </span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">)
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">typeTransform </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">x</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">]) </span><span style="font-style:italic;color:#8be9fd;">=&gt;</span><span style="color:#f8f8f2;"> transform[</span><span style="font-style:italic;color:#66d9ef;">T</span><span style="color:#f8f8f2;">](x</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">data)
  
  label andThen hot andThen typeTransform
</span><span style="color:#6272a4;">///////////////////////////////////////////////////////

</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">dataLoader </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">TextLoader</span><span style="color:#f8f8f2;">(</span><span style="color:#6be5fd;">Path</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">of(</span><span style="color:#f1fa8c;">&quot;data&quot;</span><span style="color:#f8f8f2;">, </span><span style="color:#f1fa8c;">&quot;Churn_Modelling.csv&quot;</span><span style="color:#f8f8f2;">))</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">load()
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">data </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> dataLoader</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">cols[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">](</span><span style="color:#bd93f9;">3</span><span style="color:#f8f8f2;">, -</span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">)

</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">encoders </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> createEncoders[</span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">](data)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">numericData </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> encoders(data)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">scaler </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">StandardScaler</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">]()</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">fit(numericData)

</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">prepareData </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">d</span><span style="color:#f8f8f2;">: </span><span style="font-style:italic;color:#66d9ef;">Tensor2D</span><span style="color:#f8f8f2;">[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">]) </span><span style="font-style:italic;color:#8be9fd;">=&gt; </span><span style="color:#ffffff;">{
  </span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">numericData </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> encoders(d)
  scaler</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">transform(numericData)
</span><span style="color:#ffffff;">}

</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">x </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> prepareData(data)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">y </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> dataLoader</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">cols[</span><span style="font-style:italic;color:#8be9fd;">Float</span><span style="color:#f8f8f2;">](-</span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">)
</span></code></pre>
<ol>
<li>First, we load raw data from a CSV file, then we select all columns between 3-rd and last one (-1 means: length - 1).</li>
<li>Initial data is of <code>String</code> type, later we choose numerical data type such as <code>Float</code>.</li>
<li>We compose label, one-hot encoders and data type transformer into a function inside the <code>createEncoders</code> function. That allows us to use <code>prepareData</code> function later for validation dataset.</li>
<li><code>y</code> data we take from the last column of the dataset.</li>
</ol>
<p>I am not going to describe the entire code of data preparation classes. The goal of encoders is to 
prepare data for deep neural network training and inference. We normalise all columns as per their individual means and 
standard deviations. Also, we encode categorical columns using <code>0</code> and <code>1</code> using <a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a> approach.</p>
<h1 id="training-run">Training Run</h1>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#f8f8f2;">((</span><span style="color:#ffffff;">xTrain</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">xTest</span><span style="color:#f8f8f2;">), (</span><span style="color:#ffffff;">yTrain</span><span style="color:#f8f8f2;">, </span><span style="color:#ffffff;">yTest</span><span style="color:#f8f8f2;">)) </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">(x, y)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">split(</span><span style="color:#bd93f9;">0.2</span><span style="font-style:italic;color:#8be9fd;">f</span><span style="color:#f8f8f2;">)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">model </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> ann</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">train(xTrain, yTrain, epochs </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">100</span><span style="color:#f8f8f2;">)
</span></code></pre>
<p>We split <code>x</code> and <code>y</code> data into 80% and 20% parts for training and testing accordingly.
Finally, we execute the training for 100 <code>epochs</code>.</p>
<pre style="background-color:#282a36;">
<code><span style="color:#50fa7b;">sbt:ann</span><span style="color:#ff79c6;">&gt;</span><span style="color:#f8f8f2;"> run
</span><span style="color:#50fa7b;">[info]</span><span style="color:#f8f8f2;"> running starter
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 1/100, avg. loss: 0.30220446, accuracy: 0.782
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 2/100, avg. loss: 0.30736533, accuracy: 0.811375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 3/100, avg. loss: 0.30326372, accuracy: 0.818125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 4/100, avg. loss: 0.30306807, accuracy: 0.818875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 5/100, avg. loss: 0.3028989, accuracy: 0.820125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 6/100, avg. loss: 0.30242646, accuracy: 0.82025
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 7/100, avg. loss: 0.3018655, accuracy: 0.8195
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 8/100, avg. loss: 0.30157945, accuracy: 0.81975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 9/100, avg. loss: 0.3014126, accuracy: 0.819875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 10/100, avg. loss: 0.30122074, accuracy: 0.819625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 11/100, avg. loss: 0.3009277, accuracy: 0.81975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 12/100, avg. loss: 0.30088165, accuracy: 0.82
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 13/100, avg. loss: 0.30078012, accuracy: 0.8205
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 14/100, avg. loss: 0.30074772, accuracy: 0.8205
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 15/100, avg. loss: 0.30070674, accuracy: 0.8205
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 16/100, avg. loss: 0.30053124, accuracy: 0.82025
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 17/100, avg. loss: 0.2976923, accuracy: 0.81975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 18/100, avg. loss: 0.2536276, accuracy: 0.84275
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 19/100, avg. loss: 0.24473017, accuracy: 0.85675
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 20/100, avg. loss: 0.24557488, accuracy: 0.857125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 21/100, avg. loss: 0.24528943, accuracy: 0.857875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 22/100, avg. loss: 0.2451054, accuracy: 0.857125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 23/100, avg. loss: 0.24494325, accuracy: 0.857375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 24/100, avg. loss: 0.24466132, accuracy: 0.857
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 25/100, avg. loss: 0.24451153, accuracy: 0.857625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 26/100, avg. loss: 0.24442412, accuracy: 0.857375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 27/100, avg. loss: 0.24431105, accuracy: 0.857625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 28/100, avg. loss: 0.24418788, accuracy: 0.857375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 29/100, avg. loss: 0.2440211, accuracy: 0.85775
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 30/100, avg. loss: 0.24400905, accuracy: 0.85725
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 31/100, avg. loss: 0.24397133, accuracy: 0.85725
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 32/100, avg. loss: 0.24386458, accuracy: 0.857375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 33/100, avg. loss: 0.24389265, accuracy: 0.8575
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 34/100, avg. loss: 0.24378827, accuracy: 0.857375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 35/100, avg. loss: 0.24381112, accuracy: 0.857875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 36/100, avg. loss: 0.2437651, accuracy: 0.857875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 37/100, avg. loss: 0.24369456, accuracy: 0.85775
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 38/100, avg. loss: 0.24377964, accuracy: 0.857625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 39/100, avg. loss: 0.2435442, accuracy: 0.85775
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 40/100, avg. loss: 0.24363366, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 41/100, avg. loss: 0.24358764, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 42/100, avg. loss: 0.24355079, accuracy: 0.858375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 43/100, avg. loss: 0.24369176, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 44/100, avg. loss: 0.24361038, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 45/100, avg. loss: 0.24359651, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 46/100, avg. loss: 0.24361634, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 47/100, avg. loss: 0.24357627, accuracy: 0.858
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 48/100, avg. loss: 0.24334462, accuracy: 0.85825
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 49/100, avg. loss: 0.24335352, accuracy: 0.858
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 50/100, avg. loss: 0.24341401, accuracy: 0.858375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 51/100, avg. loss: 0.24324806, accuracy: 0.8585
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 52/100, avg. loss: 0.24296027, accuracy: 0.858
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 53/100, avg. loss: 0.24271448, accuracy: 0.85775
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 54/100, avg. loss: 0.24256946, accuracy: 0.85825
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 55/100, avg. loss: 0.24257207, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 56/100, avg. loss: 0.24284393, accuracy: 0.8585
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 57/100, avg. loss: 0.2430726, accuracy: 0.85825
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 58/100, avg. loss: 0.2431463, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 59/100, avg. loss: 0.24277006, accuracy: 0.857625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 60/100, avg. loss: 0.2423336, accuracy: 0.8585
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 61/100, avg. loss: 0.24251764, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 62/100, avg. loss: 0.24255769, accuracy: 0.858625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 63/100, avg. loss: 0.2427412, accuracy: 0.85825
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 64/100, avg. loss: 0.2428449, accuracy: 0.85825
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 65/100, avg. loss: 0.24228723, accuracy: 0.858625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 66/100, avg. loss: 0.24231568, accuracy: 0.85875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 67/100, avg. loss: 0.24237442, accuracy: 0.858125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 68/100, avg. loss: 0.24238351, accuracy: 0.8585
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 69/100, avg. loss: 0.24219948, accuracy: 0.859125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 70/100, avg. loss: 0.24231845, accuracy: 0.858875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 71/100, avg. loss: 0.24243066, accuracy: 0.85875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 72/100, avg. loss: 0.2423754, accuracy: 0.859
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 73/100, avg. loss: 0.24225388, accuracy: 0.8585
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 74/100, avg. loss: 0.2420498, accuracy: 0.858875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 75/100, avg. loss: 0.24199313, accuracy: 0.858625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 76/100, avg. loss: 0.2420193, accuracy: 0.858875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 77/100, avg. loss: 0.24175513, accuracy: 0.85875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 78/100, avg. loss: 0.24191435, accuracy: 0.859625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 79/100, avg. loss: 0.2418117, accuracy: 0.85925
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 80/100, avg. loss: 0.24193105, accuracy: 0.859125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 81/100, avg. loss: 0.24175763, accuracy: 0.859375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 82/100, avg. loss: 0.24183328, accuracy: 0.859375
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 83/100, avg. loss: 0.24171984, accuracy: 0.85975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 84/100, avg. loss: 0.2419013, accuracy: 0.859125
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 85/100, avg. loss: 0.24182202, accuracy: 0.859625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 86/100, avg. loss: 0.24179217, accuracy: 0.859875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 87/100, avg. loss: 0.2416485, accuracy: 0.859875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 88/100, avg. loss: 0.24175707, accuracy: 0.86
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 89/100, avg. loss: 0.24161652, accuracy: 0.85975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 90/100, avg. loss: 0.24164297, accuracy: 0.8595
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 91/100, avg. loss: 0.24179684, accuracy: 0.859625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 92/100, avg. loss: 0.2417788, accuracy: 0.859875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 93/100, avg. loss: 0.24164356, accuracy: 0.859875
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 94/100, avg. loss: 0.24161309, accuracy: 0.8595
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 95/100, avg. loss: 0.24144296, accuracy: 0.85975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 96/100, avg. loss: 0.24163213, accuracy: 0.85975
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 97/100, avg. loss: 0.24149604, accuracy: 0.86
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 98/100, avg. loss: 0.24137497, accuracy: 0.85925
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 99/100, avg. loss: 0.24164252, accuracy: 0.859625
</span><span style="color:#50fa7b;">epoch:</span><span style="color:#f8f8f2;"> 100/100, avg. loss: 0.24153009, accuracy: 0.85975
</span><span style="color:#50fa7b;">training</span><span style="color:#f8f8f2;"> time: 5.654 in sec
</span></code></pre>
<p>We can see that <code>accuracy</code> is increasing quite quick. Also, loss value is becoming stable already after 20 epochs.</p>
<p>Entire training for 8000 data samples takes less than 6 seconds.</p>
<h1 id="testing">Testing</h1>
<h2 id="single-test">Single Test</h2>
<pre style="background-color:#282a36;">
<code><span style="color:#6272a4;">// Single test
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">example </span><span style="color:#ff79c6;">= </span><span style="color:#6be5fd;">TextLoader</span><span style="color:#f8f8f2;">(
  </span><span style="color:#f1fa8c;">&quot;n/a,n/a,n/a,600,France,Male,40,3,60000,2,1,1,50000,n/a&quot;
</span><span style="color:#f8f8f2;">)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">cols[</span><span style="font-style:italic;color:#66d9ef;">String</span><span style="color:#f8f8f2;">](</span><span style="color:#bd93f9;">3</span><span style="color:#f8f8f2;">, -</span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">testExample </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> prepareData(example)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">exited </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> 
  predictedToBinary(model</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">predict(testExample)</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">as1D</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">data</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">head) </span><span style="color:#ff79c6;">== </span><span style="color:#bd93f9;">1</span><span style="color:#f8f8f2;">
println(</span><span style="color:#8be9fd;">s</span><span style="color:#f1fa8c;">&quot;Exited customer? </span><span style="color:#ffffff;">$exited</span><span style="color:#f1fa8c;">&quot;</span><span style="color:#f8f8f2;">)
</span></code></pre>
<p>We use <code>predict</code> method on the model to test prediction on the single data sample:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#50fa7b;">Exited</span><span style="color:#f8f8f2;"> customer</span><span style="color:#ff79c6;">?</span><span style="color:#f8f8f2;"> false
</span></code></pre><h2 id="dataset-test">Dataset Test</h2>
<p>We have left 20% of the initial data for testing purposes. So now we can check trained model accuracy on new data 
that model had never seen before:</p>
<pre style="background-color:#282a36;">
<code><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">testPredicted </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> model</span><span style="color:#ff79c6;">.</span><span style="color:#f8f8f2;">predict(xTest)
</span><span style="font-style:italic;color:#8be9fd;">val </span><span style="color:#ffffff;">value </span><span style="color:#ff79c6;">=</span><span style="color:#f8f8f2;"> accuracy(yTest, testPredicted)
println(</span><span style="color:#8be9fd;">s</span><span style="color:#f1fa8c;">&quot;test accuracy = </span><span style="color:#ffffff;">$value</span><span style="color:#f1fa8c;">&quot;</span><span style="color:#f8f8f2;">)  
</span></code></pre>
<p>Model accuracy on unseen data is quite as well:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#50fa7b;">test</span><span style="color:#f8f8f2;"> accuracy = 0.8625
</span></code></pre><h1 id="python-implementation">Python Implementation</h1>
<p>Almost the same implementation in Python takes much longer to train the model. 
Although, we are using a bit more advanced optimizer such as <code>Adam</code>.</p>
<p>Here is the code snippet that starts model training:</p>
<pre style="background-color:#282a36;">
<code><span style="color:#f8f8f2;">ann</span><span style="color:#ff79c6;">.</span><span style="color:#50fa7b;">compile</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#ffb86c;">optimizer </span><span style="color:#ff79c6;">= </span><span style="color:#f1fa8c;">&#39;adam&#39;</span><span style="color:#f8f8f2;">, \
  </span><span style="font-style:italic;color:#ffb86c;">loss </span><span style="color:#ff79c6;">= </span><span style="color:#f1fa8c;">&#39;binary_crossentropy&#39;</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">metrics </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">[</span><span style="color:#f1fa8c;">&#39;accuracy&#39;</span><span style="color:#f8f8f2;">])

</span><span style="color:#ff79c6;">import </span><span style="color:#f8f8f2;">time
start </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">time</span><span style="color:#ff79c6;">.</span><span style="color:#50fa7b;">process_time</span><span style="color:#f8f8f2;">()
ann</span><span style="color:#ff79c6;">.</span><span style="color:#50fa7b;">fit</span><span style="color:#f8f8f2;">(X_train, y_train, </span><span style="font-style:italic;color:#ffb86c;">batch_size </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">32</span><span style="color:#f8f8f2;">, </span><span style="font-style:italic;color:#ffb86c;">epochs </span><span style="color:#ff79c6;">= </span><span style="color:#bd93f9;">100</span><span style="color:#f8f8f2;">)
end </span><span style="color:#ff79c6;">= </span><span style="color:#f8f8f2;">time</span><span style="color:#ff79c6;">.</span><span style="color:#50fa7b;">process_time</span><span style="color:#f8f8f2;">() </span><span style="color:#ff79c6;">- </span><span style="color:#f8f8f2;">start
</span><span style="color:#8be9fd;">print</span><span style="color:#f8f8f2;">(</span><span style="font-style:italic;color:#8be9fd;">f</span><span style="color:#f1fa8c;">&quot;training time = </span><span style="color:#f8f8f2;">{end}</span><span style="color:#f1fa8c;"> sec&quot;</span><span style="color:#f8f8f2;">)
</span></code></pre><pre style="background-color:#282a36;">
<code><span style="color:#50fa7b;">training</span><span style="color:#f8f8f2;"> time = 24.495086
</span></code></pre>
<p>It is almost 5 times longer. I knew that Python is slow language.</p>
<p>See full code here: <a href="https://github.com/novakov-alexey/tensorflow-ann-python/blob/main/artificial_neural_network.py">tensorflow-ann-python</a></p>
<h1 id="summary">Summary</h1>
<p>We have seen that Scala implementation looks very concise thanks to the great language design.
It also works faster than Python implementation in Keras on CPU.</p>
<p>Artificial Neural Network can be understood by newbies as magic, 
but a closer look shows that basic building blocks are just math.</p>
<p>Being a functional programmer in Scala, you may find that some of the code is not doing total functions, but partial functions by throwing erros. 
Unfortunatelly, there are several possibilities where user may provide
wrong inputs from math perspective, so that we could return an <a href="https://www.scala-lang.org/api/current/scala/util/Either.html">Either.Left</a> or something like that to mitigate this problem.</p>
<p>Making a machine learning library is fun, since it is just algorithms and in-memory computations.
You do not need to deal with that much I/O, networking or distributed systems programming.
Main part even does not parallelise, so no concurrency and cognitive overhead related with it. 
Of course, to make an ML library today for real life, you would require <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">support of GPU</a> for faster, parallel computation.</p>
<p><strong>Implemented code demonstrates two points:</strong></p>
<ol>
<li>ANN is an algorithm that you can implement yourself in any programming language. No magic is involved.</li>
<li>Scala is a perfect language to implement libraries for data science and machine learning.</li>
</ol>
<h1 id="source-code">Source Code</h1>
<p>Entire code of the Scala ANN implementation can be found here:</p>
<p><a href="https://github.com/novakov-alexey/deep-learning-scala">https://github.com/novakov-alexey/deep-learning-scala</a></p>
<h1 id="reference-links">Reference Links</h1>
<ul>
<li>
<p><a href="https://www.bogotobogo.com/python/scikit-learn/Artificial-Neural-Network-ANN-4-Backpropagation.php">Artificial-Neural-Network-ANN-4-Backpropagation</a></p>
</li>
<li>
<p><a href="https://riptutorial.com/machine-learning/example/31623/backpropagation---the-heart-of-neural-networks">backpropagation---the-heart-of-neural-networks</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms</a></p>
</li>
</ul>

      
  </article>
</section>

       

      <footer>
        
          
<figure class="mini_logo ">
    <a href="https:&#x2F;&#x2F;novakov-alexey.github.io" style="background-image: url(https:&#x2F;&#x2F;novakov-alexey.github.io/img/alexey_white2.jpg)"></a>
</figure>
<h5>
    <a href="https:&#x2F;&#x2F;novakov-alexey.github.io">Software Engineering Notes</a>
</h5>

          
<ul class="social_list foot_list" >
    
    
    <li class="button extra_small font_faint"><a href="https://github.com/novakov-alexey" target="_blank" >novakov-alexey</a><i class="fab fa-github" ></i></li>
    
    
    <li class="button extra_small font_faint"><a href="https://twitter.com/alexey_novakov" target="_blank">@alexey_novakov</a><i class="fab fa-twitter" ></i></li>
    
    
    
    
    <li class="button extra_small font_faint"><a href="https://www.linkedin.com/in/alexey-novakov-26468624" target="_blank">alexey-novakov-26468624</a><i class="fab fa-linkedin" ></i></li>
    
    
    
    
    <!-- 
    <li class="button extra_small font_faint"><a href="https:&#x2F;&#x2F;novakov-alexey.github.io/rss.xml" target ="_blank">rss</a><i class="fas fa-rss" ></i></li>
     -->
</ul>

        
      </footer>
    </body>
</html>
